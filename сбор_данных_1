import requests 
from bs4 import BeautifulSoup 

headers = { 
    'Host': 'hh.ru', 
    'User-Agent': 'Safari', 
    'Accept': '*/*', 
    'Accept-Encoding': 'gzip, deflate, br', 
    'Connection': 'keep-alive' 
} 

def parse_page(page_number, items_per_page): 
    url = f"https://hh.ru/search/vacancy?text=python&items_on_page={items_per_page}&page={page_number}" 
    response = requests.get(url, headers=headers) 
    soup = BeautifulSoup(response.text, 'html.parser') 

    vacancies = [] 
    # Поиск элементов по классу, содержащему информацию о вакансиях 
    vacancy_items = soup.find_all('div', class_='serp-item serp-item_link vacancy-serp-item_clickme') 
    for item in vacancy_items: 
        title = item.find('span', class_='serp-item__title-link serp-item__title') 
        company = item.find('div', class_='vacancy-serp-item__meta-info-company') 
        experience = item.find('div', class_='bloko-h-spacing-container bloko-h-spacing-container_base-0') 
        compensation = item.find('span', class_='vacancy-serp__vacancy-compensation') 
        apply_link = item.find('a', class_='bloko-button bloko-button_kind-success bloko-button_scale-small') 
      

        vacancies.append({ 
            'title': title.get_text(strip=True) if title else None, 
            'company': company.get_text(strip=True) if company else None, 
            'experience': experience.get_text(strip=True) if experience else None, 
            'compensation': compensation.get_text(strip=True) if compensation else None, 
            'apply_link': apply_link['href'] if apply_link else None 
        }) 

    return vacancies 

# Собираем данные со страниц от 1 до 20 
all_vacancies = [] 
for page in range(1, 21): 
    all_vacancies.extend(parse_page(page, 20)) 

for vacancy in all_vacancies: 
    print(vacancy)
